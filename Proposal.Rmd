---
title: "Open vs. closed innovation: using online network data to measure innovation"
author: "Benjamin Snow and Oliver Bott"
date: "24 October 2014"
output:
  html_document:
    fig_caption: yes
    number_sections: no
  pdf_document:
    fig_caption: yes
    number_sections: yes
bibliography:
    - Packages.bib
    - Main.bib
---

============

```{r include=FALSE}
# Set working directory. Change as needed
setwd('C:/Users/a6p/Desktop/Uni 2014/E1161 - Collaborative Research/Final Project/')

# Load packages and create BibTeX file
# Note: must have repmis, rsdmx, httr, dplyr and rjson packages installed
PackagesUsed <- c("ggplot2", "repmis", "rsdmx", "httr", "dplyr", "rjson")

# Load PackagesUsed and create .bib BibTeX file
repmis::LoadandCite(PackagesUsed, file = "Packages.bib", install = FALSE)
```




## Introduction

Policy makers worldwide have a profound interest in innovation for its significance for economic development and prospertiy. @taylor2004 views innovation as "the driving force behind modern economic growth, relative industrial power, and competitive advantage" (p.222). @schumpeter1942 greatly shaped the innovation discourse with the concept of structural change and the business cycle, the process he termed "creative destruction". Numerous studies, for example the Innovation Union Scoreboard[^IUS] and the OECD Science, Technology and Innovation Scoreboard[^STI], have attempted to measure and compare the innovation performance on the national level.  However, until today most examinations of innovation have put their analytical emphasis on national level patent data, relying on this form of registering of proprietary data as a means of measuring innovation.  This leaves largely unexplored other, more open measures of innovation. The relatively recent emergence of network-based research systems offer new and potentially more instructive metrics by which to measure innovation, compared to protected closed knowledge. 

This research project examines the potential preferability of using collaborative online network data on the city-level as an innovation indicator. By doing so, this work will critically assess current innovation indicators in the hope of offering new alternatives for measuring and understanding innovation. Since @freeman2009, among other scholars, called for the continuous improvement of innovation measurement, this work seeks to go beyond the widespread use of patent data to contribute to the refinement of innovation indicators, and the field as a whole.

## State of the Field

**Defining Innovation**

Using a rather grand view in his understanding of innovation, @schumpeter1942 sees innovation as "a process of industrial mutation, that incessantly revolutionizes the economic structure from within". In a more understated characterization, @smith2005 defines innovation as "the creation of something qualitatively new, via processes of learning and knowledge building. It involves changing competences and capabilities, and producing qualitatively new performance outcomes" [@smith2005, 149]. While it is widely accepted that innovation can take many forms, e.g. product, process, marketing and proccess innovations, @frankelius2009, in his extensive literature review of innovation studies, criticizes the widespread underlying assumption that innovation is limited to technological innovation. While accepting @frankelius2009's critique of innovation as taking place outside of the technological realm, for the purpose of this study technological innovation, and specifically software innovation, will be the primary focus following relatively closely to @smith2005's definition.

**Measuring Innovation**

The frequent technological focus when measuring innovation can partly be explained by the difficulties associated with innovation's measurement. @smith2005 notes the measuring challenge, as innovation is by definition a novelty and thus commensurability is a demanding task. For these reasons innovation has traditionally though controversially been measured by looking either at its inputs, outputs, our throughputs. Attempting to measure patents by inputs often focuses on resources, such as personnel and equipment allocated to R&D, which @freeman2009 notes is often overestimating innovation in research and development by including the routine with the novel. @freeman2009 compares this with output oriented measures, which are often based on what he concludes are the already inadequate measures such as GDP. Other output oriented attempts include other economic outputs in the form of patents, product output and related revenues [@freeman2009]. 

An indicator most often found in innovation research is patent data [see @taylor2004]. A patent is a "temporary legal monopoly granted by the government to an inventor for the commerical use of his or her invention [@taylor2004, 229). A patent constitutes a property right awarded when an invention is shown to be non-trivial, useful, and novel [@taylor2004, 230]. Patents were first used to measure demand-side determinants of innovation by Scherer and Schmookler [REFERENCE], and have been used in the analysis of innovation activity for over three decades [@taylor2004, 230). While long hindered by technological limitations requiring labor intensive patent analysis, recent improvements due to machine-readable patent data have spurred recent econometric analysis, as did for example @hall2005 in their large-scale patent analysis. @taylor2004 uses patent data taken from 1963 to 1999 in six different industries and their future citation levels and uses Ordinary Least Squares (OLS) model to test what he terms the 'industry-innovation assumption'. 

**Limitations of Patent Data**

Despite the usefulness of patent data, @taylor2004 found several limitations. In addition to the 'classification problem' related to assigning a specific industries to patents, patents may vary widely in significance, both technically and economically [@taylor2004, 231]. Most significantly for the purpose on this study, @taylor2004 as well as @pakes1980 found that "patents are a flawed measure particularly since not all new innovations are patented and since patents differ greatly in their economic impact” (@taylor2004, 378). Thus, while for some considerable time patents have been considered to be the most effective proxy with which to measure innovation, even recent studies have begun to examine alternatives due to patents limitations in measuring innovation. This is why for example @taylor2004 also used publication data and the number of their citations as an innovation proxy. Still, both data on patents and academic publishing include only proprietary or closed forms of innovation.

**Using Network Data as Innovation Indicator**

Current developments in research indicate that "characteristics that were important last century may well no longer be so relevant today and indeed may even be positively misleading" [@freeman2009, 3]. Today we witness a shift away from the belief that innovation only occurs in professional R&D labs, a change towards what @freeman2009 calls "research without frontiers" (p.13). Even though networks and research collaborations become increasingly important, there have been relatively few studies focusing on network data [see for example @breschi2005]. Even where research networks have been analysed, the focus is too often on economically useful knowledge [see @acs2002]. Other studies focusing on research networks focus on other protected collaborative networks [see @ponds2010]. In an exception to this standard, @Senghore2014 and team attempt to answer whether social network statistics act as indicators of innovation performance within a network, and which statistics could predict innovation performance.  Using @Gnyawali2013's model on cluster and network effects to analyze miltipartite social networks at mass collaboration events, gathering their data from NASA's International Space Apps Challenge.  They use graph theory models constructed from affiliation networks in the same challenge over two different years, where 63 curated projects represent nodes, and submitted solutions represent edges, finding (preliminarily) that distributions likely correlate to key aspects, thought at the point of publication the manner in which correlation is measured is not mentioned (@Senghore2014).

Since @freeman2009 among others called for the continuous improvement of innovation measurement, this work seeks to contribute to the refinement of innovation indicators. The purpose of this study is to explore the conceptual and statistical viability of a new metric by which we can measure innovation. New metrics are needed, because "too much energy has gone into squeezing the last bit of juice out of old data collected for different purposes relative to the design of new types of data" (Kenneth Arrow, quoted from Smith 2005, p. 148).

In light of the above mentioned state of innovation research we want to examine the following research question:  
**To what extent can open innovation network data add to the measurement of innovation performance?**
Technological advances related to the increasing use of the internet and open research platforms like GitHub, we want to explore whether open knowledge networks can help refine currently limited innovation performance measurements.

# Methodology

To examin open network data against patent data, this study will rely largely on two data sets and use the statistical tool R [@CiteR] for the data analysis.

**API network data**

The first data set is obtained by using the Application Programming Interface (commonly known as an API) data for open networks. To examine open data innovation, data will be obtained from the the git repository web-based hosting service GitHub[^GIT]. GitHub is a web-based hosting service used for collaborative research, allowing researchers to simultaneously work on projects. Its use of distributed revision control and source code management make it a commonly used software development collaboration tool. Since most of the repositories are openly accessible we can use API tools to track the popularity of repositories, measured by the repository user counts. The R [@CiteR] packages @R-httr, @R-dplyr and @R-rjson allow us to compile data on the user counts and locations associated with different repositories. We are working on the underlying assumption that work done in GitHub repositories is openly accessible innovation.

For the analysis, first we will create location vectors for different cities with the `locations` code. Since many GitHub users can be located, we will identify different open innovation clusters, for example Berlin, New York and San Francisco. In addition, we use the `vector()` code to get information on user counts, focusing on repositories with more than 20 or so followers. By combining locations and user counts data by using `data.frame()`, we will be able to construct data sets for different location clusters and user count numbers, where users of highly relevant repositories are located.

**Closed innovation OECD patent data**

```{r include=FALSE}
# Install packages to pull OECD data
# Note must have devtools package installed.
require("devtools")
library("rsdmx")
```

For closed innovation we use city-level patent data, taken from the Organization for Economic Co-operation and Development[^OECD]. The R [@CiteR] package [@R-rsdmx] necessary for obtaining the OECD dataset. Patent Cooperation Treaty (PCT) patent data are used to track internationally patentend inventions. For our preliminary sample, we took patent data of 20 cities overall, ranging from six different countries, including their country level patent data, for general comparison over the time period 2000 until 2012. We pull the OECD data from the online database using [@R-rsdmx]. As we are interested in patent data, we work with data indicating the PCT patent applications per 10,000 inhabitants. From the same database, we also use GDP per capita data and environmental data, as other variables which could prove significant in explaining differences in innovation. The OECD data will be in a format that allows us to locate patent activity to individual cities. 

The code to pull OECD into R is shown below:

```{r include=FALSE}
# Pull OECD city patent data
dataURL <- "http://stats.oecd.org/restsdmx/sdmx.ashx/GetData/CITIES/DEU+DE001+DE002+DE003+DE007+DE035+DE507+ESP+ES001+ES002+FRA+FR001+ITA+IT001+IT002+IT009+SWE+SE001+SE002+SE003+USA+US014+US048+US084+US146+US234.PCT_INTENSITY?startTime=2000&endTime=2012"

sdmx <- readSDMX(dataURL)
city_pat <- as.data.frame(sdmx)

# Pull OECD GDP per capita data

dataURL2 <- "http://stats.oecd.org/restsdmx/sdmx.ashx/GetData/CITIES/DEU+DE001+DE002+DE003+DE007+DE035+DE507+ESP+ES001+ES002+FRA+FR001+ITA+IT001+IT002+IT009+SWE+SE001+SE002+SE003+USA+US014+US048+US084+US146+US234.GDP_PC?startTime=2000&endTime=2012"

sdmx <- readSDMX(dataURL2)
city_gdp <- as.data.frame(sdmx)

# Pull OECD environmental data

dataURL3 <- "http://stats.oecd.org/restsdmx/sdmx.ashx/GetData/CITIES/DEU+DE001+DE002+DE003+DE007+DE035+DE507+ESP+ES001+ES002+FRA+FR001+ITA+IT001+IT002+IT009+SWE+SE001+SE002+SE003+USA+US014+US048+US084+US146+US234.GREEN_AREA_PC?startTime=2000&endTime=2008"

sdmx <- readSDMX(dataURL3)
city_env <- as.data.frame(sdmx)
```

To clean this data for effective statistical comparison, the deletion of several empty rows, and the renaming of relevant variables with their city name rather than an attributed code, for clarity.  Past this, all datasets must be effectively merged.

```{r include=FALSE}
# Clean table by deleting and renaming rows
city_pat$VAR <- NULL # Delete columns
city_pat$attrs.df <- NULL

colnames(city_pat) <- c("METRO_ID" , "Year" , "Patent_Intensity") # Rename columns

# Clean table by deleting and renaming rows
city_gdp$VAR <- NULL # Delete columns
city_gdp$attrs.df <- NULL

colnames(city_gdp) <- c("METRO_ID" , "Year" , "GDP") # Rename columns

sdmx <- readSDMX(dataURL3)
city_env <- as.data.frame(sdmx)

# Clean table by deleting and renaming rows
city_env$VAR <- NULL # Delete columns
city_env$attrs.df <- NULL

colnames(city_env) <- c("METRO_ID" , "Year" , "GreenArea") # Rename columns

# Merge all 3 OECD citydata sets
oecd_pat_gdp <- merge(city_pat , city_gdp , by=c("METRO_ID" , "Year"))
oecd <- merge(oecd_pat_gdp , city_env , by=c("METRO_ID" , "Year"))

# Turn Year into numeric
sapply(oecd, mode)
transform(oecd, Year = as.numeric(Year))
```

To further clean up the table we rename the city and country IDs using this code:

```{r echo=FALSE}
# Example code to further clean table
oecd$METRO_ID <- replace(oecd$METRO_ID , oecd$METRO_ID == "DEU" , "Germany")
```

**Descriptive Statistics**

To give a brief overview of the OECD data already obtained, the summary statistics give some first impression of the OECD city data set.


```{r echo=FALSE}
# Summary statistics for the innovation data
summary(oecd$Patent_Intensity ,  digits = 3)
```

In the data set, the mean for PCT patent application per 10,000 inhabitants is 3.44. The histogram indicates that the mean is not very meaningful. There are many cities with patent applications much lower and some cities with around 5.5 patent applications. This finding supports our claim that on the regional level there are great variations in patent activity, which supports our approach of focusing on city-level data. The histogram indicates a weak relationship between GDP and patent intensity.

```{r, echo=FALSE}
hist(oecd$Patent_Intensity,
      main = 'PCT applications',
      xlab = 'PCT patent applications per 10,000 inhabitants')
```


**Statistical Model**

On the type of analysis and question, this study will use an ordinary least squares (OLS) model to examine the relationship between patent and open data. If their is a relationship this would presumably show that open data shows the same innovation patent data show, but shows the 'throughput' of innovation rather than the 'output'. Open data having a relationship to patent data would presumably show innovation as a throughput, since it is measured by people finding those contributing in open data as innovators (followers on github), rather than looking at the specific innovation at completion (patents).  


**Preliminary Statistics**

> Early statistical analysis findings and expected types of statistical analysis based upon nature of datasets used


[^IUS]: For the latest edition see [http://ec.europa.eu/enterprise/policies/innovation/policy/innovation-scoreboard/index_en.htm].
[^STI]: For the latest edition see [http://www.oecd.org/sti/scoreboard.htm].
[^OECD]: Online accessible on [http://stats.oecd.org].
[^GIT]: Online accessilbe on [https://github.com/].
